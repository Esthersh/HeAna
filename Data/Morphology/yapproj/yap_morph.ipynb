{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "1. Run YAP API server\n",
    "1. `pip install -r requirements.txt`\n",
    "1. Run NEMO API server `uvicorn api_main:app --port 8090`\n",
    "1. Have a look at the swagger OpenAPI documentation by opening http://localhost:8090/docs in your browser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/home/esther/PycharmProjects/ANLP_analogies/Data/Datasets/AnalogiesData.csv\")\n",
    "sentences = data[['base_description','option_1_desc','option_2_desc','option_3_desc','option_4_desc']]\n",
    "# Concatenate all cells in each column\n",
    "texts = sentences.values.flatten().tolist()\n",
    "texts = [x for x in texts if type(x)==str]\n",
    "\n",
    "file = open('sentences.txt','w')\n",
    "for s in texts:\n",
    "    if type(s)==str:\n",
    "        file.write(s+\"\\n\")\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def run_yap(text):\n",
    "    localhost_yap = \"http://localhost:8000/yap/heb/joint\"\n",
    "    data = json.dumps({'text': \"{}  \".format(text)})  # input string ends with two space characters\n",
    "    headers = {'content-type': 'application/json'}\n",
    "    response = requests.get(url=localhost_yap, data=data, headers=headers)\n",
    "    json_response = response.json()\n",
    "    keys = list(json_response.keys())\n",
    "    return json_response.get(keys[1])\n",
    "\n",
    "\n",
    "def df_from_yap_output(output, text, res, is_word):\n",
    "    add_data = pd.DataFrame(columns = [\"sentence\", \"word\", \"POS\"])\n",
    "    for row in output.split('\\n'):\n",
    "        morph_list = row.split('\\t')\n",
    "        if len(morph_list) < 6:\n",
    "            continue\n",
    "        assert morph_list[4] == morph_list[5]\n",
    "        if is_word:\n",
    "            new_row = {\"sentence\": \"\", \"word\": morph_list[2], \"POS\": morph_list[5]}\n",
    "        else:\n",
    "            new_row = {\"sentence\": text, \"word\": morph_list[2], \"POS\": morph_list[5]}\n",
    "        new_row = pd.DataFrame(new_row, index=[0])\n",
    "        add_data = pd.concat([add_data, new_row], ignore_index=True)\n",
    "    return add_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"yap_morph_1_outof_5.csv\", index_col=0)\n",
    "df2 = pd.read_csv(\"yap_morph_2_outof_6.csv\", index_col=0)\n",
    "df3 = pd.read_csv(\"yap_morph_3_outof_9.csv\", index_col=0)\n",
    "df4 = pd.read_csv(\"yap_morph_4_outof_10.csv\", index_col=0)\n",
    "df5 = pd.read_csv(\"yap_morph_5_outof_10.csv\", index_col=0)\n",
    "df6 = pd.read_csv(\"yap_morph_6_outof_10.csv\", index_col=0)\n",
    "yap_morph = pd.concat([df1, df2, df3, df4, df5, df6])\n",
    "yap_morph.to_csv(\"yap_morph.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
